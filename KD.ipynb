{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ginGh5xtCvex"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import argparse\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_once(msg: str):\n",
        "    rank = int(os.environ.get(\"RANK\", \"0\"))\n",
        "    if rank == 0:\n",
        "        print(msg, flush=True)\n",
        "\n",
        "\n",
        "def find_pad_token(tokenizer: AutoTokenizer):\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def format_prompt(instruction: str, inp: Optional[str], answer: str) -> str:\n",
        "    if inp and len(inp.strip()) > 0:\n",
        "        prefix = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n\"\n",
        "    else:\n",
        "        prefix = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
        "    return prefix + answer\n",
        "\n",
        "\n",
        "def get_4bit_config():\n",
        "    return BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "1WPPUuZ1CzjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(dataset_id: str, tokenizer: AutoTokenizer, max_len: int, limit_train: Optional[int] = None):\n",
        "    raw = load_dataset(dataset_id)\n",
        "\n",
        "    def _format(row):\n",
        "        text = format_prompt(row[\"instruction\"], row.get(\"input\", \"\"), row[\"output\"])\n",
        "        return {\"text\": text + tokenizer.eos_token}\n",
        "\n",
        "    ds = raw.map(_format, remove_columns=raw[\"train\"].column_names)\n",
        "\n",
        "    def _tok(batch):\n",
        "        return tokenizer(batch[\"text\"], truncation=True, max_length=max_len)\n",
        "\n",
        "    ds = ds.map(_tok, batched=True, remove_columns=[\"text\"])\n",
        "    if limit_train is not None and limit_train > 0:\n",
        "        ds[\"train\"] = ds[\"train\"].select(range(min(limit_train, len(ds[\"train\"]))))\n",
        "\n",
        "    return ds\n"
      ],
      "metadata": {
        "id": "2vPffWQJC2Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_teacher(teacher_id: str):\n",
        "    print_once(f\"Loading TEACHER: {teacher_id}\")\n",
        "    teacher = AutoModelForCausalLM.from_pretrained(\n",
        "        teacher_id,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=get_4bit_config(),\n",
        "    ).eval()\n",
        "    for p in teacher.parameters():\n",
        "        p.requires_grad_(False)\n",
        "    return teacher\n",
        "\n",
        "\n",
        "def load_student(student_id: str, lora_r: int, lora_alpha: int, lora_dropout: float):\n",
        "    print_once(f\"Loading STUDENT: {student_id}\")\n",
        "    student = AutoModelForCausalLM.from_pretrained(\n",
        "        student_id,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=get_4bit_config(),\n",
        "    )\n",
        "    student = prepare_model_for_kbit_training(student)\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=lora_r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        lora_dropout=lora_dropout,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    )\n",
        "    student = get_peft_model(student, lora_cfg)\n",
        "    return student\n"
      ],
      "metadata": {
        "id": "llOaZND_C4UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shift_logits_and_labels(logits: torch.Tensor, labels: torch.Tensor):\n",
        "    # Causal LM predicts next token â†’ align predictions and targets by shifting\n",
        "    shift_logits = logits[:, :-1, :].contiguous()\n",
        "    shift_labels = labels[:, 1:].contiguous()\n",
        "    return shift_logits, shift_labels\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class KDTrainer(Trainer):\n",
        "    teacher: AutoModelForCausalLM = None\n",
        "    alpha: float = 0.5\n",
        "    temperature: float = 1.0\n",
        "    ce_loss: torch.nn.Module = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "    kl_loss: torch.nn.Module = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs[\"labels\"]\n",
        "        outputs_s = model(**inputs)\n",
        "        logits_s = outputs_s.logits\n",
        "\n",
        "        # --- CE: student vs ground-truth labels ---\n",
        "        s_logits, s_labels = shift_logits_and_labels(logits_s, labels)\n",
        "        loss_ce = self.ce_loss(s_logits.view(-1, s_logits.size(-1)), s_labels.view(-1))\n",
        "\n",
        "        # --- KL: student vs teacher (soft targets) ---\n",
        "        with torch.no_grad():\n",
        "            t_out = self.teacher(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
        "            logits_t = t_out.logits\n",
        "\n",
        "        s_logits_t, _ = shift_logits_and_labels(logits_s, labels)\n",
        "        t_logits_t, _ = shift_logits_and_labels(logits_t, labels)\n",
        "\n",
        "        T = self.temperature\n",
        "        s_logp = torch.log_softmax(s_logits_t / T, dim=-1)  # student log-probs\n",
        "        t_p = torch.softmax(t_logits_t / T, dim=-1)         # teacher probs\n",
        "\n",
        "        loss_kl = self.kl_loss(s_logp, t_p) * (T * T)\n",
        "\n",
        "        loss = self.alpha * loss_ce + (1.0 - self.alpha) * loss_kl\n",
        "        return (loss, outputs_s) if return_outputs else loss\n"
      ],
      "metadata": {
        "id": "GPWrXfDQC823"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"LLM Knowledge Distillation (SFT + KL) with QLoRA\")\n",
        "    parser.add_argument(\"--teacher\", type=str, default=\"Qwen/Qwen2.5-7B-Instruct\")\n",
        "    parser.add_argument(\"--student\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    parser.add_argument(\"--dataset\", type=str, default=\"yahma/alpaca-cleaned\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"student_kd_out\")\n",
        "\n",
        "    parser.add_argument(\"--alpha\", type=float, default=0.5, help=\"Weight for CE; (1-alpha) used for KL\")\n",
        "    parser.add_argument(\"--temperature\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--epochs\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--lr\", type=float, default=2e-4)\n",
        "    parser.add_argument(\"--bsz\", type=int, default=4)\n",
        "    parser.add_argument(\"--grad_accum\", type=int, default=8)\n",
        "    parser.add_argument(\"--max_len\", type=int, default=1024)\n",
        "    parser.add_argument(\"--limit_train\", type=int, default=0, help=\"If >0, limit training examples\")\n",
        "\n",
        "    parser.add_argument(\"--lora_r\", type=int, default=16)\n",
        "    parser.add_argument(\"--lora_alpha\", type=int, default=32)\n",
        "    parser.add_argument(\"--lora_dropout\", type=float, default=0.05)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Tokenizer (use student's for training)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.student, use_fast=True, trust_remote_code=True)\n",
        "    tokenizer = find_pad_token(tokenizer)\n",
        "\n",
        "    # Data\n",
        "    print_once(f\"Loading dataset: {args.dataset}\")\n",
        "    ds = build_dataset(args.dataset, tokenizer, args.max_len, limit_train=(args.limit_train or None))\n",
        "\n",
        "    # Models\n",
        "    teacher = load_teacher(args.teacher)\n",
        "    student = load_student(args.student, args.lora_r, args.lora_alpha, args.lora_dropout)\n",
        "\n",
        "    # Collator\n",
        "    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "    # Trainer\n",
        "    train_args = TrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        per_device_train_batch_size=args.bsz,\n",
        "        gradient_accumulation_steps=args.grad_accum,\n",
        "        learning_rate=args.lr,\n",
        "        num_train_epochs=args.epochs,\n",
        "        bf16=True,\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    trainer = KDTrainer(\n",
        "        model=student,\n",
        "        args=train_args,\n",
        "        train_dataset=ds[\"train\"],\n",
        "        data_collator=collator,\n",
        "        teacher=teacher,\n",
        "        alpha=args.alpha,\n",
        "        temperature=args.temperature,\n",
        "    )\n",
        "\n",
        "    print_once(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    print_once(\"Saving model + tokenizer...\")\n",
        "    trainer.save_model(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "    print_once(f\"Done. Saved to: {args.output_dir}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "DfetAqgTC-iS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}